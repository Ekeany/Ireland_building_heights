{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108405, 78)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>tile</th>\n",
       "      <th>Height</th>\n",
       "      <th>2020-2020_001-365_HL_TSA_SEN2L_NDV_STM_B0007_GRD</th>\n",
       "      <th>2020-2020_001-365_HL_TSA_SEN2L_NDV_STM_B0007_ERO</th>\n",
       "      <th>2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0011_OPN_asc</th>\n",
       "      <th>2020-2020_001-365_HL_TSA_SEN2L_BNR_STM_B0002_CLS</th>\n",
       "      <th>2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0003_DIL_asc</th>\n",
       "      <th>2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0003_GRD_asc</th>\n",
       "      <th>...</th>\n",
       "      <th>2020-2020_001-365_HL_TSA_SEN2L_SW1_STM_B0004_ERO</th>\n",
       "      <th>2020-2020_001-365_HL_TSA_SEN2L_RE1_STM_B0011_ERO</th>\n",
       "      <th>2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0001_GRD_asc</th>\n",
       "      <th>2020-2020_001-365_HL_TSA_SEN2L_RE1_STM_B0011_OPN</th>\n",
       "      <th>2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0001_CLS_asc</th>\n",
       "      <th>2020-2020_001-365_HL_TSA_SEN2L_TCB_STM_B0004_ERO</th>\n",
       "      <th>2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0001_ERO</th>\n",
       "      <th>2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0007_OPN_asc</th>\n",
       "      <th>2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0007_GRD_asc</th>\n",
       "      <th>2020-2020_001-365_HL_TSA_SEN2L_RE2_STM_B0008_ERO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2169</td>\n",
       "      <td>2555</td>\n",
       "      <td>X0002_Y0002</td>\n",
       "      <td>2</td>\n",
       "      <td>871</td>\n",
       "      <td>4532</td>\n",
       "      <td>637</td>\n",
       "      <td>1399</td>\n",
       "      <td>-15294</td>\n",
       "      <td>2960</td>\n",
       "      <td>...</td>\n",
       "      <td>1562</td>\n",
       "      <td>108</td>\n",
       "      <td>6968</td>\n",
       "      <td>139</td>\n",
       "      <td>-16088</td>\n",
       "      <td>3262</td>\n",
       "      <td>1179</td>\n",
       "      <td>-4890</td>\n",
       "      <td>9657</td>\n",
       "      <td>1716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1801</td>\n",
       "      <td>2273</td>\n",
       "      <td>X0002_Y0002</td>\n",
       "      <td>2</td>\n",
       "      <td>1666</td>\n",
       "      <td>7551</td>\n",
       "      <td>436</td>\n",
       "      <td>3366</td>\n",
       "      <td>-11690</td>\n",
       "      <td>6530</td>\n",
       "      <td>...</td>\n",
       "      <td>1183</td>\n",
       "      <td>45</td>\n",
       "      <td>10847</td>\n",
       "      <td>85</td>\n",
       "      <td>-15535</td>\n",
       "      <td>2506</td>\n",
       "      <td>1720</td>\n",
       "      <td>-9474</td>\n",
       "      <td>6407</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2146</td>\n",
       "      <td>2572</td>\n",
       "      <td>X0002_Y0002</td>\n",
       "      <td>2</td>\n",
       "      <td>6869</td>\n",
       "      <td>1659</td>\n",
       "      <td>599</td>\n",
       "      <td>1960</td>\n",
       "      <td>-15671</td>\n",
       "      <td>2827</td>\n",
       "      <td>...</td>\n",
       "      <td>1318</td>\n",
       "      <td>144</td>\n",
       "      <td>5829</td>\n",
       "      <td>187</td>\n",
       "      <td>-17193</td>\n",
       "      <td>2245</td>\n",
       "      <td>873</td>\n",
       "      <td>-7623</td>\n",
       "      <td>4714</td>\n",
       "      <td>1378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2153</td>\n",
       "      <td>2572</td>\n",
       "      <td>X0002_Y0002</td>\n",
       "      <td>2</td>\n",
       "      <td>4207</td>\n",
       "      <td>4321</td>\n",
       "      <td>514</td>\n",
       "      <td>1795</td>\n",
       "      <td>-16108</td>\n",
       "      <td>1520</td>\n",
       "      <td>...</td>\n",
       "      <td>1315</td>\n",
       "      <td>99</td>\n",
       "      <td>2162</td>\n",
       "      <td>240</td>\n",
       "      <td>-17150</td>\n",
       "      <td>2321</td>\n",
       "      <td>1383</td>\n",
       "      <td>-6802</td>\n",
       "      <td>4239</td>\n",
       "      <td>1602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2129</td>\n",
       "      <td>2522</td>\n",
       "      <td>X0002_Y0002</td>\n",
       "      <td>2</td>\n",
       "      <td>4905</td>\n",
       "      <td>3122</td>\n",
       "      <td>511</td>\n",
       "      <td>1678</td>\n",
       "      <td>-14739</td>\n",
       "      <td>1802</td>\n",
       "      <td>...</td>\n",
       "      <td>1287</td>\n",
       "      <td>135</td>\n",
       "      <td>3075</td>\n",
       "      <td>380</td>\n",
       "      <td>-15910</td>\n",
       "      <td>2326</td>\n",
       "      <td>1286</td>\n",
       "      <td>-5849</td>\n",
       "      <td>5496</td>\n",
       "      <td>1652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      X     Y         tile  Height  \\\n",
       "0  2169  2555  X0002_Y0002       2   \n",
       "1  1801  2273  X0002_Y0002       2   \n",
       "2  2146  2572  X0002_Y0002       2   \n",
       "3  2153  2572  X0002_Y0002       2   \n",
       "4  2129  2522  X0002_Y0002       2   \n",
       "\n",
       "   2020-2020_001-365_HL_TSA_SEN2L_NDV_STM_B0007_GRD  \\\n",
       "0                                               871   \n",
       "1                                              1666   \n",
       "2                                              6869   \n",
       "3                                              4207   \n",
       "4                                              4905   \n",
       "\n",
       "   2020-2020_001-365_HL_TSA_SEN2L_NDV_STM_B0007_ERO  \\\n",
       "0                                              4532   \n",
       "1                                              7551   \n",
       "2                                              1659   \n",
       "3                                              4321   \n",
       "4                                              3122   \n",
       "\n",
       "   2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0011_OPN_asc  \\\n",
       "0                                                637      \n",
       "1                                                436      \n",
       "2                                                599      \n",
       "3                                                514      \n",
       "4                                                511      \n",
       "\n",
       "   2020-2020_001-365_HL_TSA_SEN2L_BNR_STM_B0002_CLS  \\\n",
       "0                                              1399   \n",
       "1                                              3366   \n",
       "2                                              1960   \n",
       "3                                              1795   \n",
       "4                                              1678   \n",
       "\n",
       "   2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0003_DIL_asc  \\\n",
       "0                                             -15294      \n",
       "1                                             -11690      \n",
       "2                                             -15671      \n",
       "3                                             -16108      \n",
       "4                                             -14739      \n",
       "\n",
       "   2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0003_GRD_asc  ...  \\\n",
       "0                                               2960     ...   \n",
       "1                                               6530     ...   \n",
       "2                                               2827     ...   \n",
       "3                                               1520     ...   \n",
       "4                                               1802     ...   \n",
       "\n",
       "   2020-2020_001-365_HL_TSA_SEN2L_SW1_STM_B0004_ERO  \\\n",
       "0                                              1562   \n",
       "1                                              1183   \n",
       "2                                              1318   \n",
       "3                                              1315   \n",
       "4                                              1287   \n",
       "\n",
       "   2020-2020_001-365_HL_TSA_SEN2L_RE1_STM_B0011_ERO  \\\n",
       "0                                               108   \n",
       "1                                                45   \n",
       "2                                               144   \n",
       "3                                                99   \n",
       "4                                               135   \n",
       "\n",
       "   2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0001_GRD_asc  \\\n",
       "0                                               6968      \n",
       "1                                              10847      \n",
       "2                                               5829      \n",
       "3                                               2162      \n",
       "4                                               3075      \n",
       "\n",
       "   2020-2020_001-365_HL_TSA_SEN2L_RE1_STM_B0011_OPN  \\\n",
       "0                                               139   \n",
       "1                                                85   \n",
       "2                                               187   \n",
       "3                                               240   \n",
       "4                                               380   \n",
       "\n",
       "   2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0001_CLS_asc  \\\n",
       "0                                             -16088      \n",
       "1                                             -15535      \n",
       "2                                             -17193      \n",
       "3                                             -17150      \n",
       "4                                             -15910      \n",
       "\n",
       "   2020-2020_001-365_HL_TSA_SEN2L_TCB_STM_B0004_ERO  \\\n",
       "0                                              3262   \n",
       "1                                              2506   \n",
       "2                                              2245   \n",
       "3                                              2321   \n",
       "4                                              2326   \n",
       "\n",
       "   2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0001_ERO  \\\n",
       "0                                              1179   \n",
       "1                                              1720   \n",
       "2                                               873   \n",
       "3                                              1383   \n",
       "4                                              1286   \n",
       "\n",
       "   2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0007_OPN_asc  \\\n",
       "0                                              -4890      \n",
       "1                                              -9474      \n",
       "2                                              -7623      \n",
       "3                                              -6802      \n",
       "4                                              -5849      \n",
       "\n",
       "   2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0007_GRD_asc  \\\n",
       "0                                               9657      \n",
       "1                                               6407      \n",
       "2                                               4714      \n",
       "3                                               4239      \n",
       "4                                               5496      \n",
       "\n",
       "   2020-2020_001-365_HL_TSA_SEN2L_RE2_STM_B0008_ERO  \n",
       "0                                              1716  \n",
       "1                                              2011  \n",
       "2                                              1378  \n",
       "3                                              1602  \n",
       "4                                              1652  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from BorutaShap import BorutaShap\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "important_features = ['X','Y','tile','Height',\n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_NDV_STM_B0007_GRD', '2020-2020_001-365_HL_TSA_SEN2L_NDV_STM_B0007_ERO', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0011_OPN_asc', '2020-2020_001-365_HL_TSA_SEN2L_BNR_STM_B0002_CLS', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0003_DIL_asc', '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0003_GRD_asc', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0011_CLS_asc', '2020-2020_001-365_HL_TSA_SEN2L_TCG_STM_B0008_OPN', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_BNR_STM_B0004_OPN', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0001_DIL_asc', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0012_DIL_asc', '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0006_GRD_asc', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_SW1_STM_B0002_ERO', '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0007_DIL_asc', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0002_GRD_asc', '2020-2020_001-365_HL_TSA_SEN2L_NDW_STM_B0001_DIL', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_BNR_STM_B0005_ERO', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0005_GRD_asc', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_GRN_STM_B0004_ERO', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0002_DIL_asc', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0012_DIL_asc', '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0001_DIL_asc', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_SW1_STM_B0012_OPN', '2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0003_ERO', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0003_ERO_asc', '2020-2020_001-365_HL_TSA_SEN2L_TCW_STM_B0012_ERO', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0005_DIL_asc', '2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0007_ERO', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_BNR_STM_B0004_ERO', '2020-2020_001-365_HL_TSA_SEN2L_TCG_STM_B0006_ERO', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0005_OPN_asc', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0007_DIL_asc', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_NDB_STM_B0007_DIL', '2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0004_ERO', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_NDW_STM_B0013_ERO', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0001_CLS_asc', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_GRN_STM_B0004_GRD', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0001_OPN_asc', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_NDW_STM_B0004_DIL', '2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0012_OPN', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_SW2_STM_B0002_ERO', '2020-2020_001-365_HL_TSA_SEN2L_TCB_STM_B0009_DIL', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0002_ERO', '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0006_CLS_asc', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_NDW_STM_B0013_OPN', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0003_DIL_asc', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_TCW_STM_B0007_GRD', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0011_CLS_desc', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0012_CLS_asc', '2020-2020_001-365_HL_TSA_SEN2L_RE2_STM_B0011_GRD', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0004_GRD_asc', '2020-2020_001-365_HL_TSA_SEN2L_TCB_STM_B0012_CLS', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0008_GRD_asc', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0012_GRD_asc', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_RE1_STM_B0001_CLS', '2020-2020_001-365_HL_TSA_SEN2L_TCG_STM_B0003_OPN', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_TCG_STM_B0003_BHT', '2020-2020_001-365_HL_TSA_SEN2L_SW1_STM_B0003_ERO', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0002_CLS', '2020-2020_001-365_HL_TSA_SEN2L_NDB_STM_B0011_CLS', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0009_ERO_asc', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0007_CLS_asc', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_SW1_STM_B0007_ERO', '2020-2020_001-365_HL_TSA_SEN2L_TCB_STM_B0013_ERO', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_SW1_STM_B0004_ERO', '2020-2020_001-365_HL_TSA_SEN2L_RE1_STM_B0011_ERO', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0001_GRD_asc', '2020-2020_001-365_HL_TSA_SEN2L_RE1_STM_B0011_OPN', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0001_CLS_asc', '2020-2020_001-365_HL_TSA_SEN2L_TCB_STM_B0004_ERO', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0001_ERO', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0007_OPN_asc', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0007_GRD_asc', '2020-2020_001-365_HL_TSA_SEN2L_RE2_STM_B0008_ERO']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "file = 'C:/Users/egnke/PythonCode/Met_Eireann_git/Ireland_building_heights/Modelling/DataSetCreation/Pixel-Wise-Data/building_height_sample_asc.csv'\n",
    "data = pd.read_csv(file)\n",
    "data = data[important_features]\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    \n",
    "    ascending_cols, descending_cols, difference_cols = get_columns(df)\n",
    "    \n",
    "    for i in range(len(ascending_cols)):\n",
    "        df[difference_cols[i]] = df[ascending_cols[i]] - df[descending_cols[i]]\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_columns(df):\n",
    "    \n",
    "    cols = df.columns\n",
    "    \n",
    "    ascending_cols = list(filter(lambda k: 'asc' in k, cols))\n",
    "    \n",
    "    descending_cols = list(filter(lambda k: 'desc' in k, cols))\n",
    "    \n",
    "    test1 = [col.replace('asc','') for col in ascending_cols]\n",
    "    test2 = [col.replace('desc','') for col in descending_cols]\n",
    "    \n",
    "    difference_cols = [col.replace('asc','difference') for col in ascending_cols]\n",
    "    return ascending_cols, descending_cols, difference_cols\n",
    "\n",
    "    \n",
    "#data = feature_engineering(data)\n",
    "#print(data.shape)\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins     = np.arange(1,41)\n",
    "bins[-1] = data.Height.max()\n",
    "data['binned'] = pd.cut(data['Height'], bins)\n",
    "data['stratified_labels'] = LabelEncoder().fit_transform(data['binned'])\n",
    "#data = data.sample(50000)\n",
    "\n",
    "y = data['Height']\n",
    "stratified = data['stratified_labels']\n",
    "X = data.drop(['X','Y','tile','Height','binned','stratified_labels'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7a34cc553a43b082b9708ca91eb713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 attributes confirmed important: ['2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0002_GRD_asc', '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0001_CLS_asc', '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0009_ERO_asc', '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0005_DIL_asc', '2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0004_ERO', '2020-2020_001-365_HL_TSA_SEN2L_TCG_STM_B0003_BHT', '2020-2020_001-365_HL_TSA_SEN2L_BNR_STM_B0002_CLS', '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0004_GRD_asc', '2020-2020_001-365_HL_TSA_SEN2L_SW1_STM_B0007_ERO', '2020-2020_001-365_HL_TSA_SEN2L_NDB_STM_B0007_DIL', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0001_GRD_asc', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0012_GRD_asc', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0001_CLS_asc', '2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0003_ERO', '2020-2020_001-365_HL_TSA_SEN2L_SW1_STM_B0003_ERO', '2020-2020_001-365_HL_TSA_SEN2L_NDW_STM_B0001_DIL', '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0011_CLS_asc', '2020-2020_001-365_HL_TSA_SEN2L_SW1_STM_B0004_ERO', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0003_DIL_asc', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0002_DIL_asc', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0007_OPN_asc', '2020-2020_001-365_HL_TSA_SEN2L_NDV_STM_B0007_ERO', '2020-2020_001-365_HL_TSA_SEN2L_RE2_STM_B0011_GRD', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0007_DIL_asc', '2020-2020_001-365_HL_TSA_SEN2L_NDW_STM_B0013_ERO', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0012_DIL_asc', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0005_OPN_asc', '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0006_CLS_asc', '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0007_DIL_asc', '2020-2020_001-365_HL_TSA_SEN2L_NDW_STM_B0013_OPN', '2020-2020_001-365_HL_TSA_SEN2L_NDB_STM_B0011_CLS', '2020-2020_001-365_HL_TSA_SEN2L_RE2_STM_B0008_ERO', '2020-2020_001-365_HL_TSA_SEN2L_TCG_STM_B0008_OPN', '2020-2020_001-365_HL_TSA_SEN2L_TCB_STM_B0013_ERO', '2020-2020_001-365_HL_TSA_SEN2L_TCG_STM_B0003_OPN', '2020-2020_001-365_HL_TSA_SEN2L_TCW_STM_B0007_GRD', '2020-2020_001-365_HL_TSA_SEN2L_TCW_STM_B0012_ERO', '2020-2020_001-365_HL_TSA_SEN2L_GRN_STM_B0004_ERO', '2020-2020_001-365_HL_TSA_SEN2L_SW2_STM_B0002_ERO', '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0001_DIL_asc', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0012_CLS_asc', '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0003_ERO_asc', '2020-2020_001-365_HL_TSA_SEN2L_SW1_STM_B0012_OPN', '2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0012_OPN', '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0012_DIL_asc', '2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0002_CLS', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0011_CLS_desc', '2020-2020_001-365_HL_TSA_SEN2L_GRN_STM_B0004_GRD', '2020-2020_001-365_HL_TSA_SEN2L_TCB_STM_B0012_CLS', '2020-2020_001-365_HL_TSA_SEN2L_TCB_STM_B0004_ERO', '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0003_DIL_asc', '2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0007_ERO', '2020-2020_001-365_HL_TSA_SEN2L_NDW_STM_B0004_DIL', '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0003_GRD_asc', '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0006_GRD_asc', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0005_GRD_asc', '2020-2020_001-365_HL_TSA_SEN2L_BNR_STM_B0004_ERO', '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0011_OPN_asc', '2020-2020_001-365_HL_TSA_SEN2L_TCG_STM_B0006_ERO', '2020-2020_001-365_HL_TSA_SEN2L_RE1_STM_B0001_CLS', '2020-2020_001-365_HL_TSA_SEN2L_RE1_STM_B0011_ERO', '2020-2020_001-365_HL_TSA_SEN2L_RE1_STM_B0011_OPN', '2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0001_ERO', '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0007_GRD_asc', '2020-2020_001-365_HL_TSA_SEN2L_NDV_STM_B0007_GRD', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0001_DIL_asc', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0001_OPN_asc', '2020-2020_001-365_HL_TSA_SEN2L_BNR_STM_B0005_ERO', '2020-2020_001-365_HL_TSA_SEN2L_TCB_STM_B0009_DIL', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0007_CLS_asc', '2020-2020_001-365_HL_TSA_SEN2L_BNR_STM_B0004_OPN', '2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0002_ERO', '2020-2020_001-365_HL_TSA_SEN2L_SW1_STM_B0002_ERO']\n",
      "0 attributes confirmed unimportant: []\n",
      "1 tentative attributes remains: ['2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0008_GRD_asc']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0002_GRD_asc</th>\n",
       "      <th>2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0001_CLS_asc</th>\n",
       "      <th>2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0009_ERO_asc</th>\n",
       "      <th>2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0005_DIL_asc</th>\n",
       "      <th>2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0004_ERO</th>\n",
       "      <th>2020-2020_001-365_HL_TSA_SEN2L_TCG_STM_B0003_BHT</th>\n",
       "      <th>2020-2020_001-365_HL_TSA_SEN2L_BNR_STM_B0002_CLS</th>\n",
       "      <th>2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0004_GRD_asc</th>\n",
       "      <th>2020-2020_001-365_HL_TSA_SEN2L_SW1_STM_B0007_ERO</th>\n",
       "      <th>2020-2020_001-365_HL_TSA_SEN2L_NDB_STM_B0007_DIL</th>\n",
       "      <th>...</th>\n",
       "      <th>2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0007_GRD_asc</th>\n",
       "      <th>2020-2020_001-365_HL_TSA_SEN2L_NDV_STM_B0007_GRD</th>\n",
       "      <th>2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0001_DIL_asc</th>\n",
       "      <th>2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0001_OPN_asc</th>\n",
       "      <th>2020-2020_001-365_HL_TSA_SEN2L_BNR_STM_B0005_ERO</th>\n",
       "      <th>2020-2020_001-365_HL_TSA_SEN2L_TCB_STM_B0009_DIL</th>\n",
       "      <th>2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0007_CLS_asc</th>\n",
       "      <th>2020-2020_001-365_HL_TSA_SEN2L_BNR_STM_B0004_OPN</th>\n",
       "      <th>2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0002_ERO</th>\n",
       "      <th>2020-2020_001-365_HL_TSA_SEN2L_SW1_STM_B0002_ERO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3323</td>\n",
       "      <td>-16088</td>\n",
       "      <td>622</td>\n",
       "      <td>-13369</td>\n",
       "      <td>2003</td>\n",
       "      <td>52</td>\n",
       "      <td>1399</td>\n",
       "      <td>3209</td>\n",
       "      <td>2038</td>\n",
       "      <td>245</td>\n",
       "      <td>...</td>\n",
       "      <td>9657</td>\n",
       "      <td>871</td>\n",
       "      <td>-2883</td>\n",
       "      <td>-9625</td>\n",
       "      <td>2287</td>\n",
       "      <td>734</td>\n",
       "      <td>295</td>\n",
       "      <td>1842</td>\n",
       "      <td>1201</td>\n",
       "      <td>1116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6750</td>\n",
       "      <td>-15535</td>\n",
       "      <td>494</td>\n",
       "      <td>-10863</td>\n",
       "      <td>2921</td>\n",
       "      <td>93</td>\n",
       "      <td>3366</td>\n",
       "      <td>6671</td>\n",
       "      <td>1370</td>\n",
       "      <td>-663</td>\n",
       "      <td>...</td>\n",
       "      <td>6407</td>\n",
       "      <td>1666</td>\n",
       "      <td>-6158</td>\n",
       "      <td>-15137</td>\n",
       "      <td>2420</td>\n",
       "      <td>733</td>\n",
       "      <td>-6436</td>\n",
       "      <td>2808</td>\n",
       "      <td>1918</td>\n",
       "      <td>765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3634</td>\n",
       "      <td>-17193</td>\n",
       "      <td>472</td>\n",
       "      <td>-15006</td>\n",
       "      <td>1602</td>\n",
       "      <td>480</td>\n",
       "      <td>1960</td>\n",
       "      <td>2557</td>\n",
       "      <td>1824</td>\n",
       "      <td>481</td>\n",
       "      <td>...</td>\n",
       "      <td>4714</td>\n",
       "      <td>6869</td>\n",
       "      <td>-8554</td>\n",
       "      <td>-10495</td>\n",
       "      <td>1705</td>\n",
       "      <td>1202</td>\n",
       "      <td>-4772</td>\n",
       "      <td>1840</td>\n",
       "      <td>966</td>\n",
       "      <td>816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1618</td>\n",
       "      <td>-17150</td>\n",
       "      <td>500</td>\n",
       "      <td>-14257</td>\n",
       "      <td>1994</td>\n",
       "      <td>0</td>\n",
       "      <td>1795</td>\n",
       "      <td>1711</td>\n",
       "      <td>1824</td>\n",
       "      <td>732</td>\n",
       "      <td>...</td>\n",
       "      <td>4239</td>\n",
       "      <td>4207</td>\n",
       "      <td>-8373</td>\n",
       "      <td>-10090</td>\n",
       "      <td>2183</td>\n",
       "      <td>1048</td>\n",
       "      <td>-2407</td>\n",
       "      <td>2284</td>\n",
       "      <td>1418</td>\n",
       "      <td>839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022</td>\n",
       "      <td>-15910</td>\n",
       "      <td>538</td>\n",
       "      <td>-13567</td>\n",
       "      <td>1795</td>\n",
       "      <td>125</td>\n",
       "      <td>1678</td>\n",
       "      <td>1989</td>\n",
       "      <td>1953</td>\n",
       "      <td>361</td>\n",
       "      <td>...</td>\n",
       "      <td>5496</td>\n",
       "      <td>4905</td>\n",
       "      <td>-7705</td>\n",
       "      <td>-9282</td>\n",
       "      <td>2294</td>\n",
       "      <td>1091</td>\n",
       "      <td>-3031</td>\n",
       "      <td>2007</td>\n",
       "      <td>1359</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0002_GRD_asc  \\\n",
       "0                                               3323      \n",
       "1                                               6750      \n",
       "2                                               3634      \n",
       "3                                               1618      \n",
       "4                                               2022      \n",
       "\n",
       "   2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0001_CLS_asc  \\\n",
       "0                                             -16088      \n",
       "1                                             -15535      \n",
       "2                                             -17193      \n",
       "3                                             -17150      \n",
       "4                                             -15910      \n",
       "\n",
       "   2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0009_ERO_asc  \\\n",
       "0                                                622      \n",
       "1                                                494      \n",
       "2                                                472      \n",
       "3                                                500      \n",
       "4                                                538      \n",
       "\n",
       "   2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0005_DIL_asc  \\\n",
       "0                                             -13369      \n",
       "1                                             -10863      \n",
       "2                                             -15006      \n",
       "3                                             -14257      \n",
       "4                                             -13567      \n",
       "\n",
       "   2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0004_ERO  \\\n",
       "0                                              2003   \n",
       "1                                              2921   \n",
       "2                                              1602   \n",
       "3                                              1994   \n",
       "4                                              1795   \n",
       "\n",
       "   2020-2020_001-365_HL_TSA_SEN2L_TCG_STM_B0003_BHT  \\\n",
       "0                                                52   \n",
       "1                                                93   \n",
       "2                                               480   \n",
       "3                                                 0   \n",
       "4                                               125   \n",
       "\n",
       "   2020-2020_001-365_HL_TSA_SEN2L_BNR_STM_B0002_CLS  \\\n",
       "0                                              1399   \n",
       "1                                              3366   \n",
       "2                                              1960   \n",
       "3                                              1795   \n",
       "4                                              1678   \n",
       "\n",
       "   2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0004_GRD_asc  \\\n",
       "0                                               3209      \n",
       "1                                               6671      \n",
       "2                                               2557      \n",
       "3                                               1711      \n",
       "4                                               1989      \n",
       "\n",
       "   2020-2020_001-365_HL_TSA_SEN2L_SW1_STM_B0007_ERO  \\\n",
       "0                                              2038   \n",
       "1                                              1370   \n",
       "2                                              1824   \n",
       "3                                              1824   \n",
       "4                                              1953   \n",
       "\n",
       "   2020-2020_001-365_HL_TSA_SEN2L_NDB_STM_B0007_DIL  ...  \\\n",
       "0                                               245  ...   \n",
       "1                                              -663  ...   \n",
       "2                                               481  ...   \n",
       "3                                               732  ...   \n",
       "4                                               361  ...   \n",
       "\n",
       "   2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0007_GRD_asc  \\\n",
       "0                                               9657      \n",
       "1                                               6407      \n",
       "2                                               4714      \n",
       "3                                               4239      \n",
       "4                                               5496      \n",
       "\n",
       "   2020-2020_001-365_HL_TSA_SEN2L_NDV_STM_B0007_GRD  \\\n",
       "0                                               871   \n",
       "1                                              1666   \n",
       "2                                              6869   \n",
       "3                                              4207   \n",
       "4                                              4905   \n",
       "\n",
       "   2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0001_DIL_asc  \\\n",
       "0                                              -2883      \n",
       "1                                              -6158      \n",
       "2                                              -8554      \n",
       "3                                              -8373      \n",
       "4                                              -7705      \n",
       "\n",
       "   2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0001_OPN_asc  \\\n",
       "0                                              -9625      \n",
       "1                                             -15137      \n",
       "2                                             -10495      \n",
       "3                                             -10090      \n",
       "4                                              -9282      \n",
       "\n",
       "   2020-2020_001-365_HL_TSA_SEN2L_BNR_STM_B0005_ERO  \\\n",
       "0                                              2287   \n",
       "1                                              2420   \n",
       "2                                              1705   \n",
       "3                                              2183   \n",
       "4                                              2294   \n",
       "\n",
       "   2020-2020_001-365_HL_TSA_SEN2L_TCB_STM_B0009_DIL  \\\n",
       "0                                               734   \n",
       "1                                               733   \n",
       "2                                              1202   \n",
       "3                                              1048   \n",
       "4                                              1091   \n",
       "\n",
       "   2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0007_CLS_asc  \\\n",
       "0                                                295      \n",
       "1                                              -6436      \n",
       "2                                              -4772      \n",
       "3                                              -2407      \n",
       "4                                              -3031      \n",
       "\n",
       "   2020-2020_001-365_HL_TSA_SEN2L_BNR_STM_B0004_OPN  \\\n",
       "0                                              1842   \n",
       "1                                              2808   \n",
       "2                                              1840   \n",
       "3                                              2284   \n",
       "4                                              2007   \n",
       "\n",
       "   2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0002_ERO  \\\n",
       "0                                              1201   \n",
       "1                                              1918   \n",
       "2                                               966   \n",
       "3                                              1418   \n",
       "4                                              1359   \n",
       "\n",
       "   2020-2020_001-365_HL_TSA_SEN2L_SW1_STM_B0002_ERO  \n",
       "0                                              1116  \n",
       "1                                               765  \n",
       "2                                               816  \n",
       "3                                               839  \n",
       "4                                              1065  \n",
       "\n",
       "[5 rows x 73 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XGBRegressor()\n",
    "\n",
    "# if classification is False it is a Regression problem\n",
    "Feature_Selector = BorutaShap(model=model,\n",
    "                              importance_measure='shap',\n",
    "                              classification=False)\n",
    "\n",
    "\n",
    "Feature_Selector.fit(X=X, y=y, n_trials=100,\n",
    "                     verbose=True, stratify=stratified)\n",
    "\n",
    "\n",
    "subset = Feature_Selector.Subset()\n",
    "subset.to_csv('subset.csv',index=False)\n",
    "subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, IsolationForest\n",
    "from sklearn.datasets import load_breast_cancer, load_boston\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.sparse import issparse\n",
    "from scipy.stats import binom_test, ks_2samp\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import os\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class BorutaShap:\n",
    "\n",
    "    \"\"\"\n",
    "    BorutaShap is a wrapper feature selection method built on the foundations of both the SHAP and Boruta algorithms.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model=None, importance_measure='Shap',\n",
    "                classification=True, percentile=100, pvalue=0.05):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        model: Model Object\n",
    "            If no model specified then a base Random Forest will be returned otherwise the specifed model will\n",
    "            be returned.\n",
    "\n",
    "        importance_measure: String\n",
    "            Which importance measure too use either Shap or Gini/Gain\n",
    "\n",
    "        classification: Boolean\n",
    "            if true then the problem is either a binary or multiclass problem otherwise if false then it is regression\n",
    "\n",
    "        percentile: Int\n",
    "            An integer ranging from 0-100 it changes the value of the max shadow importance values. Thus, lowering its value\n",
    "            would make the algorithm more lenient.\n",
    "\n",
    "        p_value: float\n",
    "            A float used as a significance level again if the p-value is increased the algorithm will be more lenient making it smaller\n",
    "            would make it more strict also by making the model more strict could impact runtime making it slower. As it will be less likley\n",
    "            to reject and accept features.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.importance_measure = importance_measure.lower()\n",
    "        self.percentile = percentile\n",
    "        self.pvalue = pvalue\n",
    "        self.classification = classification\n",
    "        self.model = model\n",
    "        self.check_model()\n",
    "\n",
    "\n",
    "    def check_model(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Checks that a model object has been passed as a parameter when intiializing the BorutaShap class.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Model Object\n",
    "            If no model specified then a base Random Forest will be returned otherwise the specifed model will\n",
    "            be returned.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        AttirbuteError\n",
    "             If the model object does not have the required attributes.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        check_fit = hasattr(self.model, 'fit')\n",
    "        check_predict_proba = hasattr(self.model, 'predict')\n",
    "\n",
    "        try:\n",
    "            check_feature_importance = hasattr(self.model, 'feature_importances_')\n",
    "\n",
    "        except:\n",
    "            check_feature_importance = True\n",
    "\n",
    "\n",
    "        if self.model is None:\n",
    "\n",
    "            if self.classification:\n",
    "                self.model = RandomForestClassifier()\n",
    "            else:\n",
    "                self.model = RandomForestRegressor()\n",
    "\n",
    "        elif check_fit is False and check_predict_proba is False:\n",
    "            raise AttributeError('Model must contain both the fit() and predict() methods')\n",
    "\n",
    "        elif check_feature_importance is False and self.importance_measure == 'gini':\n",
    "            raise AttributeError('Model must contain the feature_importances_ method to use Gini try Shap instead')\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "    def check_X(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Checks that the data passed to the BorutaShap instance is a pandas Dataframe\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Datframe\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        AttirbuteError\n",
    "             If the data is not of the expected type.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(self.X, pd.DataFrame) is False:\n",
    "            raise AttributeError('X must be a pandas Dataframe')\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "    def missing_values_y(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Checks for missing values in target variable.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Boolean\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        AttirbuteError\n",
    "             If data is not in the expected format.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(self.y, pd.Series):\n",
    "            return self.y.isnull().any().any()\n",
    "\n",
    "        elif isinstance(self.y, np.ndarray):\n",
    "            return np.isnan(self.y).any()\n",
    "\n",
    "        else:\n",
    "            raise AttributeError('Y must be a pandas Dataframe or a numpy array')\n",
    "\n",
    "\n",
    "    def check_missing_values(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Checks for missing values in the data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Boolean\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        AttirbuteError\n",
    "             If there are missing values present.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        X_missing = self.X.isnull().any().any()\n",
    "        Y_missing = self.missing_values_y()\n",
    "\n",
    "        models_to_check = ('xgb', 'catboost', 'lgbm', 'lightgbm')\n",
    "\n",
    "        model_name = str(type(self.model)).lower()\n",
    "        if X_missing or Y_missing:\n",
    "\n",
    "            if any([x in model_name for x in models_to_check]):\n",
    "                print('Warning there are missing values in your data !')\n",
    "\n",
    "            else:\n",
    "                raise ValueError('There are missing values in your Data')\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "    def Check_if_chose_train_or_test_and_train_model(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Decides to fit the model to either the training data or the test/unseen data a great discussion on the\n",
    "        differences can be found here.\n",
    "\n",
    "        https://compstat-lmu.github.io/iml_methods_limitations/pfi-data.html#introduction-to-test-vs.training-data\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if self.train_or_test.lower() == 'test':\n",
    "            # keeping the same naming convenetion as to not add complexit later on\n",
    "            self.X_boruta_train, self.X_boruta_test, self.y_train, self.y_test = train_test_split(self.X_boruta,\n",
    "                                                                                            self.y,\n",
    "                                                                                            test_size=0.3,\n",
    "                                                                                            random_state=self.random_state,\n",
    "                                                                                            stratify=self.stratify)\n",
    "            self.Train_model(self.X_boruta_train, self.y_train)\n",
    "\n",
    "        elif self.train_or_test.lower() == 'train':\n",
    "            # model will be trained and evaluated on the same data\n",
    "            self.Train_model(self.X_boruta, self.y)\n",
    "\n",
    "        else:\n",
    "            raise ValueError('The train_or_test parameter can only be \"train\" or \"test\"')\n",
    "\n",
    "\n",
    "\n",
    "    def Train_model(self, X, y):\n",
    "\n",
    "        \"\"\"\n",
    "        Trains Model also checks to see if the model is an instance of catboost as it needs extra parameters\n",
    "        also the try except is for models with a verbose statement\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: Dataframe\n",
    "            A pandas dataframe of the features.\n",
    "\n",
    "        y: Series/ndarray\n",
    "            A pandas series or numpy ndarray of the target\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        fitted model object\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if 'catboost' in str(type(self.model)).lower():\n",
    "            self.model.fit(X, y, cat_features = self.X_categorical,  verbose=False)\n",
    "\n",
    "        else:\n",
    "\n",
    "            try:\n",
    "                self.model.fit(X, y, verbose=False)\n",
    "\n",
    "            except:\n",
    "                self.model.fit(X, y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, X, y, n_trials = 20, random_state=0, sample=False,\n",
    "            train_or_test = 'test', normalize=True, verbose=True, stratify=None):\n",
    "\n",
    "        \"\"\"\n",
    "        The main body of the program this method it computes the following\n",
    "\n",
    "        1. Extend the information system by adding copies of all variables (the information system\n",
    "        is always extended by at least 5 shadow attributes, even if the number of attributes in\n",
    "        the original set is lower than 5).\n",
    "\n",
    "        2. Shuffle the added attributes to remove their correlations with the response.\n",
    "\n",
    "        3. Run a random forest classifier on the extended information system and gather the\n",
    "        Z scores computed.\n",
    "\n",
    "        4. Find the maximum Z score among shadow attributes (MZSA), and then assign a hit to\n",
    "        every attribute that scored better than MZSA.\n",
    "\n",
    "        5. For each attribute with undetermined importance perform a two-sided test of equality\n",
    "        with the MZSA.\n",
    "\n",
    "        6. Deem the attributes which have importance significantly lower than MZSA as ‘unimportant’\n",
    "        and permanently remove them from the information system.\n",
    "\n",
    "        7. Deem the attributes which have importance significantly higher than MZSA as ‘important’.\n",
    "\n",
    "        8. Remove all shadow attributes.\n",
    "\n",
    "        9. Repeat the procedure until the importance is assigned for all the attributes, or the\n",
    "        algorithm has reached the previously set limit of the random forest runs.\n",
    "\n",
    "        10. Stores results.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: Dataframe\n",
    "            A pandas dataframe of the features.\n",
    "\n",
    "        y: Series/ndarray\n",
    "            A pandas series or numpy ndarray of the target\n",
    "\n",
    "        random_state: int\n",
    "            A random state for reproducibility of results\n",
    "\n",
    "        Sample: Boolean\n",
    "            if true then a rowise sample of the data will be used to calculate the feature importance values\n",
    "\n",
    "        sample_fraction: float\n",
    "            The sample fraction of the original data used in calculating the feature importance values only\n",
    "            used if Sample==True.\n",
    "\n",
    "        train_or_test: string\n",
    "            Decides whether the feature importance should be calculated on out of sample data see the dicussion here.\n",
    "            https://compstat-lmu.github.io/iml_methods_limitations/pfi-data.html#introduction-to-test-vs.training-data\n",
    "\n",
    "        normalize: boolean\n",
    "            if true the importance values will be normalized using the z-score formula\n",
    "\n",
    "        verbose: Boolean\n",
    "            a flag indicator to print out all the rejected or accepted features.\n",
    "\n",
    "        stratify: array\n",
    "            allows the train test splits to be stratified based on given values.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        np.random.seed(random_state)\n",
    "        self.starting_X = X\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n_trials = n_trials\n",
    "        self.random_state = random_state\n",
    "        self.ncols = self.X.shape[1]\n",
    "        self.all_columns = self.X.columns.to_numpy()\n",
    "        self.rejected_columns = []\n",
    "        self.accepted_columns = []\n",
    "\n",
    "        self.check_X()\n",
    "        self.check_missing_values()\n",
    "        self.sample = sample\n",
    "        self.train_or_test = train_or_test\n",
    "        self.stratify = stratify\n",
    "\n",
    "        self.features_to_remove = []\n",
    "        self.hits  = np.zeros(self.ncols)\n",
    "        self.order = self.create_mapping_between_cols_and_indices()\n",
    "        self.create_importance_history()\n",
    "\n",
    "        if self.sample: self.preds = self.isolation_forest(self.X)\n",
    "\n",
    "        for trial in tqdm(range(self.n_trials)):\n",
    "\n",
    "            self.remove_features_if_rejected()\n",
    "            self.columns = self.X.columns.to_numpy()\n",
    "            self.create_shadow_features()\n",
    "\n",
    "            # early stopping\n",
    "            if self.X.shape[1] == 0:\n",
    "                break\n",
    "\n",
    "            else:\n",
    "\n",
    "                self.Check_if_chose_train_or_test_and_train_model()\n",
    "\n",
    "                self.X_feature_import, self.Shadow_feature_import = self.feature_importance(normalize=normalize)\n",
    "                self.update_importance_history()\n",
    "                hits = self.calculate_hits()\n",
    "                self.hits += hits\n",
    "                self.history_hits = np.vstack((self.history_hits, self.hits))\n",
    "                self.test_features(iteration=trial+1)\n",
    "\n",
    "        self.store_feature_importance()\n",
    "        self.calculate_rejected_accepted_tentative(verbose=verbose)\n",
    "\n",
    "\n",
    "    def calculate_rejected_accepted_tentative(self, verbose):\n",
    "\n",
    "        \"\"\"\n",
    "        Figures out which features have been either accepted rejeected or tentative\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        3 lists\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.rejected  = list(set(self.flatten_list(self.rejected_columns))-set(self.flatten_list(self.accepted_columns)))\n",
    "        self.accepted  = list(set(self.flatten_list(self.accepted_columns)))\n",
    "        self.tentative = list(set(self.all_columns) - set(self.rejected + self.accepted))\n",
    "\n",
    "        if verbose:\n",
    "            print(str(len(self.accepted))  + ' attributes confirmed important: ' + str(self.accepted))\n",
    "            print(str(len(self.rejected))  + ' attributes confirmed unimportant: ' + str(self.rejected))\n",
    "            print(str(len(self.tentative)) + ' tentative attributes remains: ' + str(self.tentative))\n",
    "\n",
    "\n",
    "\n",
    "    def create_importance_history(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Creates a dataframe object to store historical feature importance scores.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Datframe\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.history_shadow = np.zeros(self.ncols)\n",
    "        self.history_x = np.zeros(self.ncols)\n",
    "        self.history_hits = np.zeros(self.ncols)\n",
    "\n",
    "\n",
    "    def update_importance_history(self):\n",
    "\n",
    "        \"\"\"\n",
    "        At each iteration update the datframe object that stores the historical feature importance scores.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Datframe\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        padded_history_shadow  = np.full((self.ncols), np.NaN)\n",
    "        padded_history_x = np.full((self.ncols), np.NaN)\n",
    "\n",
    "        for (index, col) in enumerate(self.columns):\n",
    "            map_index = self.order[col]\n",
    "            padded_history_shadow[map_index] = self.Shadow_feature_import[index]\n",
    "            padded_history_x[map_index] = self.X_feature_import[index]\n",
    "\n",
    "        self.history_shadow = np.vstack((self.history_shadow, padded_history_shadow))\n",
    "        self.history_x = np.vstack((self.history_x, padded_history_x))\n",
    "\n",
    "\n",
    "\n",
    "    def store_feature_importance(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Reshapes the columns in the historical feature importance scores object also adds the mean, median, max, min\n",
    "        shadow feature scores.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Datframe\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.history_x = pd.DataFrame(data=self.history_x,\n",
    "                                 columns=self.all_columns)\n",
    "\n",
    "\n",
    "        self.history_x['Max_Shadow']    =  [max(i) for i in self.history_shadow]\n",
    "        self.history_x['Min_Shadow']    =  [min(i) for i in self.history_shadow]\n",
    "        self.history_x['Mean_Shadow']   =  [np.nanmean(i) for i in self.history_shadow]\n",
    "        self.history_x['Median_Shadow'] =  [np.nanmedian(i) for i in self.history_shadow]\n",
    "\n",
    "\n",
    "    def results_to_csv(self, filename='feature_importance'):\n",
    "\n",
    "        \"\"\"\n",
    "        Saves the historical feature importance scores to csv.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filname : string\n",
    "            used as the name for the outputed file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        comma delimnated file\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        features = pd.DataFrame(data={'Features':self.history_x.iloc[1:].columns.values,\n",
    "        'Average Feature Importance':self.history_x.iloc[1:].mean(axis=0).values,\n",
    "        'Standard Deviation Importance':self.history_x.iloc[1:].std(axis=0).values})\n",
    "\n",
    "        decision_mapper = self.create_mapping_of_features_to_attribute(maps=['Tentative','Rejected','Accepted', 'Shadow'])\n",
    "        features['Decision'] = features['Features'].map(decision_mapper)\n",
    "        features = features.sort_values(by='Average Feature Importance',ascending=False)\n",
    "\n",
    "        features.to_csv(filename + '.csv', index=False)\n",
    "\n",
    "\n",
    "    def remove_features_if_rejected(self):\n",
    "\n",
    "        \"\"\"\n",
    "        At each iteration if a feature has been rejected by the algorithm remove it from the process\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if len(self.features_to_remove) != 0:\n",
    "            for feature in self.features_to_remove:\n",
    "                try:\n",
    "                    self.X.drop(feature, axis = 1, inplace=True)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def average_of_list(lst):\n",
    "        return sum(lst) / len(lst)\n",
    "\n",
    "    @staticmethod\n",
    "    def flatten_list(array):\n",
    "        return [item for sublist in array for item in sublist]\n",
    "\n",
    "\n",
    "    def create_mapping_between_cols_and_indices(self):\n",
    "        return dict(zip(self.X.columns.to_list(), np.arange(self.X.shape[1])))\n",
    "\n",
    "\n",
    "    def calculate_hits(self):\n",
    "\n",
    "        \"\"\"\n",
    "        If a features importance is greater than the maximum importance value of all the random shadow\n",
    "        features then we assign it a hit.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Percentile : value ranging from 0-1\n",
    "            can be used to reduce value of the maximum value of the shadow features making the algorithm\n",
    "            more lenient.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        shadow_threshold = np.percentile(self.Shadow_feature_import,\n",
    "                                        self.percentile)\n",
    "\n",
    "        padded_hits = np.zeros(self.ncols)\n",
    "        hits = self.X_feature_import > shadow_threshold\n",
    "\n",
    "        for (index, col) in enumerate(self.columns):\n",
    "            map_index = self.order[col]\n",
    "            padded_hits[map_index] += hits[index]\n",
    "\n",
    "        return padded_hits\n",
    "\n",
    "\n",
    "    def create_shadow_features(self):\n",
    "        \"\"\"\n",
    "        Creates the random shadow features by shuffling the existing columns.\n",
    "\n",
    "        Returns:\n",
    "            Datframe with random permutations of the original columns.\n",
    "        \"\"\"\n",
    "        self.X_shadow = self.X.apply(np.random.permutation)\n",
    "        \n",
    "        if isinstance(self.X_shadow, pd.DataFrame):\n",
    "            # append\n",
    "            obj_col = self.X_shadow.select_dtypes(\"object\").columns.tolist()\n",
    "            if obj_col ==[] :\n",
    "                 pass\n",
    "            else :\n",
    "                 self.X_shadow[obj_col] =self.X_shadow[obj_col].astype(\"category\")\n",
    "\n",
    "        self.X_shadow.columns = ['shadow_' + feature for feature in self.X.columns]\n",
    "        self.X_boruta = pd.concat([self.X, self.X_shadow], axis = 1)\n",
    "\n",
    "        col_types = self.X_boruta.dtypes\n",
    "        self.X_categorical = list(col_types[(col_types=='category' ) | (col_types=='object')].index)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_Zscore(array):\n",
    "        \"\"\"\n",
    "        Calculates the Z-score of an array\n",
    "\n",
    "        Parameters\n",
    "         ----------\n",
    "        array: array_like\n",
    "\n",
    "        Returns:\n",
    "            normalised array\n",
    "        \"\"\"\n",
    "        mean_value = np.mean(array)\n",
    "        std_value  = np.std(array)\n",
    "        return [(element-mean_value)/std_value for element in array]\n",
    "\n",
    "\n",
    "    def feature_importance(self, normalize):\n",
    "\n",
    "        \"\"\"\n",
    "        Caculates the feature importances scores of the model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        importance_measure: string\n",
    "            allows the user to choose either the Shap or Gini importance metrics\n",
    "\n",
    "        normalize: boolean\n",
    "            if true the importance values will be normalized using the z-score formula\n",
    "\n",
    "        Returns:\n",
    "            array of normalized feature importance scores for both the shadow and original features.\n",
    "\n",
    "        Raise\n",
    "        ----------\n",
    "            ValueError:\n",
    "                If no Importance measure was specified\n",
    "        \"\"\"\n",
    "\n",
    "        if self.importance_measure == 'shap':\n",
    "\n",
    "            self.explain()\n",
    "            vals = self.shap_values\n",
    "\n",
    "            if normalize:\n",
    "                vals = self.calculate_Zscore(vals)\n",
    "\n",
    "            X_feature_import = vals[:len(self.X.columns)]\n",
    "            Shadow_feature_import = vals[len(self.X_shadow.columns):]\n",
    "\n",
    "\n",
    "        elif self.importance_measure == 'gini':\n",
    "\n",
    "                feature_importances_ =  np.abs(self.model.feature_importances_)\n",
    "\n",
    "                if normalize:\n",
    "                    feature_importances_ = self.calculate_Zscore(feature_importances_)\n",
    "\n",
    "                X_feature_import = feature_importances_[:len(self.X.columns)]\n",
    "                Shadow_feature_import = feature_importances_[len(self.X.columns):]\n",
    "\n",
    "        else:\n",
    "\n",
    "            raise ValueError('No Importance_measure was specified select one of (shap, gini)')\n",
    "\n",
    "\n",
    "        return X_feature_import, Shadow_feature_import\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def isolation_forest(X):\n",
    "        '''\n",
    "        fits isloation forest to the dataset and gives an anomally score to every sample\n",
    "        '''\n",
    "        clf = IsolationForest().fit(X)\n",
    "        preds = clf.score_samples(X)\n",
    "        return preds\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_5_percent(num):\n",
    "        return round(5  / 100 * num)\n",
    "\n",
    "\n",
    "    def get_5_percent_splits(self, length):\n",
    "        '''\n",
    "        splits dataframe into 5% intervals\n",
    "        '''\n",
    "        five_percent = self.get_5_percent(length)\n",
    "        return np.arange(five_percent,length,five_percent)\n",
    "\n",
    "\n",
    "\n",
    "    def find_sample(self):\n",
    "        '''\n",
    "        Finds a sample by comparing the distributions of the anomally scores between the sample and the original\n",
    "        distribution using the KS-test. Starts of a 5% howver will increase to 10% and then 15% etc. if a significant sample can not be found\n",
    "        '''\n",
    "        loop = True\n",
    "        iteration = 0\n",
    "        size = self.get_5_percent_splits(self.X.shape[0])\n",
    "        element = 1\n",
    "        while loop:\n",
    "\n",
    "            sample_indices = choice(np.arange(self.preds.size),  size=size[element], replace=False)\n",
    "            sample = np.take(self.preds, sample_indices)\n",
    "            if ks_2samp(self.preds, sample).pvalue > 0.95:\n",
    "                break\n",
    "\n",
    "            if iteration == 20:\n",
    "                element  += 1\n",
    "                iteration = 0\n",
    "\n",
    "\n",
    "        return self.X_boruta.iloc[sample_indices]\n",
    "\n",
    "\n",
    "\n",
    "    def explain(self):\n",
    "\n",
    "        \"\"\"\n",
    "        The shap package has numerous variants of explainers which use different assumptions depending on the model\n",
    "        type this function allows the user to choose explainer\n",
    "\n",
    "        Returns:\n",
    "            shap values\n",
    "\n",
    "        Raise\n",
    "        ----------\n",
    "            ValueError:\n",
    "                if no model type has been specified tree as default\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        explainer = shap.TreeExplainer(self.model, feature_perturbation = \"tree_path_dependent\")\n",
    "\n",
    "\n",
    "        if self.sample:\n",
    "\n",
    "\n",
    "            if self.classification:\n",
    "                # for some reason shap returns values wraped in a list of length 1\n",
    "\n",
    "                self.shap_values = np.array(explainer.shap_values(self.find_sample()))\n",
    "                if isinstance(self.shap_values, list):\n",
    "\n",
    "                    class_inds = range(len(self.shap_values))\n",
    "                    shap_imp = np.zeros(self.shap_values[0].shape[1])\n",
    "                    for i, ind in enumerate(class_inds):\n",
    "                        shap_imp += np.abs(self.shap_values[ind]).mean(0)\n",
    "                    self.shap_values /= len(self.shap_values)\n",
    "\n",
    "                elif len(self.shap_values.shape) == 3:\n",
    "                    self.shap_values = np.abs(self.shap_values).sum(axis=0)\n",
    "                    self.shap_values = self.shap_values.mean(0)\n",
    "\n",
    "                else:\n",
    "                    self.shap_values = np.abs(self.shap_values).mean(0)\n",
    "\n",
    "            else:\n",
    "                self.shap_values = explainer.shap_values(self.find_sample())\n",
    "                self.shap_values = np.abs(self.shap_values).mean(0)\n",
    "\n",
    "        else:\n",
    "\n",
    "            if self.classification:\n",
    "                # for some reason shap returns values wraped in a list of length 1\n",
    "                self.shap_values = np.array(explainer.shap_values(self.X_boruta))\n",
    "                if isinstance(self.shap_values, list):\n",
    "\n",
    "                    class_inds = range(len(self.shap_values))\n",
    "                    shap_imp = np.zeros(self.shap_values[0].shape[1])\n",
    "                    for i, ind in enumerate(class_inds):\n",
    "                        shap_imp += np.abs(self.shap_values[ind]).mean(0)\n",
    "                    self.shap_values /= len(self.shap_values)\n",
    "\n",
    "                elif len(self.shap_values.shape) == 3:\n",
    "                    self.shap_values = np.abs(self.shap_values).sum(axis=0)\n",
    "                    self.shap_values = self.shap_values.mean(0)\n",
    "\n",
    "                else:\n",
    "                    self.shap_values = np.abs(self.shap_values).mean(0)\n",
    "\n",
    "            else:\n",
    "                self.shap_values = explainer.shap_values(self.X_boruta)\n",
    "                self.shap_values = np.abs(self.shap_values).mean(0)\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def binomial_H0_test(array, n, p, alternative):\n",
    "        \"\"\"\n",
    "        Perform a test that the probability of success is p.\n",
    "        This is an exact, two-sided test of the null hypothesis\n",
    "        that the probability of success in a Bernoulli experiment is p\n",
    "        \"\"\"\n",
    "        return [binom_test(x, n=n, p=p, alternative=alternative) for x in array]\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def symetric_difference_between_two_arrays(array_one, array_two):\n",
    "        set_one = set(array_one)\n",
    "        set_two = set(array_two)\n",
    "        return np.array(list(set_one.symmetric_difference(set_two)))\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def find_index_of_true_in_array(array):\n",
    "        length = len(array)\n",
    "        return list(filter(lambda x: array[x], range(length)))\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def bonferoni_corrections(pvals, alpha=0.05, n_tests=None):\n",
    "        \"\"\"\n",
    "        used to counteract the problem of multiple comparisons.\n",
    "        \"\"\"\n",
    "        pvals = np.array(pvals)\n",
    "\n",
    "        if n_tests is None:\n",
    "            n_tests = len(pvals)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        alphacBon = alpha / float(n_tests)\n",
    "        reject = pvals <= alphacBon\n",
    "        pvals_corrected = pvals * float(n_tests)\n",
    "        return reject, pvals_corrected\n",
    "\n",
    "\n",
    "    def test_features(self, iteration):\n",
    "\n",
    "        \"\"\"\n",
    "        For each feature with an undetermined importance perform a two-sided test of equality\n",
    "        with the maximum shadow value to determine if it is statistcally better\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        hits: an array which holds the history of the number times\n",
    "              this feature was better than the maximum shadow\n",
    "\n",
    "        Returns:\n",
    "            Two arrays of the names of the accepted and rejected columns at that instance\n",
    "        \"\"\"\n",
    "\n",
    "        acceptance_p_values = self.binomial_H0_test(self.hits,\n",
    "                                                    n=iteration,\n",
    "                                                    p=0.5,\n",
    "                                                    alternative='greater')\n",
    "\n",
    "        regect_p_values = self.binomial_H0_test(self.hits,\n",
    "                                                n=iteration,\n",
    "                                                p=0.5,\n",
    "                                                alternative='less')\n",
    "\n",
    "        # [1] as function returns a tuple\n",
    "        modified_acceptance_p_values = self.bonferoni_corrections(acceptance_p_values,\n",
    "                                                                  alpha=0.05,\n",
    "                                                                  n_tests=len(self.columns))[1]\n",
    "\n",
    "        modified_regect_p_values = self.bonferoni_corrections(regect_p_values,\n",
    "                                                              alpha=0.05,\n",
    "                                                              n_tests=len(self.columns))[1]\n",
    "\n",
    "        # Take the inverse as we want true to keep featrues\n",
    "        rejected_columns = np.array(modified_regect_p_values) < self.pvalue\n",
    "        accepted_columns = np.array(modified_acceptance_p_values) < self.pvalue\n",
    "\n",
    "        rejected_indices = self.find_index_of_true_in_array(rejected_columns)\n",
    "        accepted_indices = self.find_index_of_true_in_array(accepted_columns)\n",
    "\n",
    "        rejected_features = self.all_columns[rejected_indices]\n",
    "        accepted_features = self.all_columns[accepted_indices]\n",
    "\n",
    "\n",
    "        self.features_to_remove = rejected_features\n",
    "\n",
    "\n",
    "        self.rejected_columns.append(rejected_features)\n",
    "        self.accepted_columns.append(accepted_features)\n",
    "\n",
    "\n",
    "    def TentativeRoughFix(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Sometimes no matter how many iterations are run a feature may neither be rejected or\n",
    "        accepted. This method is used in this case to make a decision on a tentative feature\n",
    "        by comparing its median importance value with the median max shadow value.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tentative: an array which holds the names of the tentative attiributes.\n",
    "\n",
    "        Returns:\n",
    "            Two arrays of the names of the final decision of the accepted and rejected columns.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        median_tentaive_values = self.history_x[self.tentative].median(axis=0).values\n",
    "        median_max_shadow = self.history_x['Max_Shadow'].median(axis=0)\n",
    "\n",
    "\n",
    "        filtered = median_tentaive_values > median_max_shadow\n",
    "\n",
    "        self.tentative = np.array(self.tentative)\n",
    "        newly_accepted = self.tentative[filtered]\n",
    "\n",
    "        if len(newly_accepted) < 1:\n",
    "            newly_rejected = self.tentative\n",
    "\n",
    "        else:\n",
    "            newly_rejected = self.symetric_difference_between_two_arrays(newly_accepted, self.tentative)\n",
    "\n",
    "        print(str(len(newly_accepted)) + ' tentative features are now accepted: ' + str(newly_accepted))\n",
    "        print(str(len(newly_rejected)) + ' tentative features are now rejected: ' + str(newly_rejected))\n",
    "\n",
    "        self.rejected = self.rejected + newly_rejected.tolist()\n",
    "        self.accepted = self.accepted + newly_accepted.tolist()\n",
    "\n",
    "\n",
    "\n",
    "    def Subset(self, tentative=False):\n",
    "        \"\"\"\n",
    "        Returns the subset of desired features\n",
    "        \"\"\"\n",
    "        if tentative:\n",
    "            return self.starting_X[self.accepted + self.tentative.tolist()]\n",
    "        else:\n",
    "            return self.starting_X[self.accepted]\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def create_list(array, color):\n",
    "        colors = [color for x in range(len(array))]\n",
    "        return colors\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_data(data, column, value):\n",
    "        data = data.copy()\n",
    "        return data.loc[(data[column] == value) | (data[column] == 'Shadow')]\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def hasNumbers(inputString):\n",
    "        return any(char.isdigit() for char in inputString)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def check_if_which_features_is_correct(my_string):\n",
    "\n",
    "        my_string = str(my_string).lower()\n",
    "        if my_string in ['tentative','rejected','accepted','all']:\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            raise ValueError(my_string + \" is not a valid value did you mean to type 'all', 'tentative', 'accepted' or 'rejected' ?\")\n",
    "\n",
    "\n",
    "\n",
    "    def plot(self, X_rotation=90, X_size=8, figsize=(12,8),\n",
    "            y_scale='log', which_features='all', display=True):\n",
    "\n",
    "        \"\"\"\n",
    "        creates a boxplot of the feature importances\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_rotation: int\n",
    "            Controls the orientation angle of the tick labels on the X-axis\n",
    "\n",
    "        X_size: int\n",
    "            Controls the font size of the tick labels\n",
    "\n",
    "        y_scale: string\n",
    "            Log transform of the y axis scale as hard to see the plot as it is normally dominated by two or three\n",
    "            features.\n",
    "\n",
    "        which_features: string\n",
    "            Despite efforts if the number of columns is large the plot becomes cluttered so this parameter allows you to\n",
    "            select subsets of the features like the accepted, rejected or tentative features default is all.\n",
    "\n",
    "        Display: Boolean\n",
    "        controls if the output is displayed or not, set to false when running test scripts\n",
    "\n",
    "        \"\"\"\n",
    "        # data from wide to long\n",
    "        data = self.history_x.iloc[1:]\n",
    "        data['index'] = data.index\n",
    "        data = pd.melt(data, id_vars='index', var_name='Methods')\n",
    "\n",
    "        decision_mapper = self.create_mapping_of_features_to_attribute(maps=['Tentative','Rejected','Accepted', 'Shadow'])\n",
    "        data['Decision'] = data['Methods'].map(decision_mapper)\n",
    "        data.drop(['index'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "        options = { 'accepted' : self.filter_data(data,'Decision', 'Accepted'),\n",
    "                    'tentative': self.filter_data(data,'Decision', 'Tentative'),\n",
    "                    'rejected' : self.filter_data(data,'Decision', 'Rejected'),\n",
    "                    'all' : data\n",
    "                    }\n",
    "\n",
    "        self.check_if_which_features_is_correct(which_features)\n",
    "        data = options[which_features.lower()]\n",
    "\n",
    "        self.box_plot(data=data,\n",
    "                      X_rotation=X_rotation,\n",
    "                      X_size=X_size,\n",
    "                      y_scale=y_scale,\n",
    "                      figsize=figsize)\n",
    "        if display:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "    def box_plot(self, data, X_rotation, X_size, y_scale, figsize):\n",
    "\n",
    "        if y_scale=='log':\n",
    "            minimum = data['value'].min()\n",
    "            if minimum <= 0:\n",
    "                data['value'] += abs(minimum) + 0.01\n",
    "\n",
    "        order = data.groupby(by=[\"Methods\"])[\"value\"].mean().sort_values(ascending=False).index\n",
    "        my_palette = self.create_mapping_of_features_to_attribute(maps= ['yellow','red','green','blue'])\n",
    "\n",
    "        # Use a color palette\n",
    "        plt.figure(figsize=figsize)\n",
    "        ax = sns.boxplot(x=data[\"Methods\"], y=data[\"value\"],\n",
    "                        order=order, palette=my_palette)\n",
    "\n",
    "        if y_scale == 'log':ax.set(yscale=\"log\")\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=X_rotation, size=X_size)\n",
    "        ax.set_title('Feature Importance')\n",
    "        ax.set_ylabel('Z-Score')\n",
    "        ax.set_xlabel('Features')\n",
    "\n",
    "\n",
    "    def create_mapping_of_features_to_attribute(self, maps = []):\n",
    "\n",
    "        rejected = list(self.rejected)\n",
    "        tentative = list(self.tentative)\n",
    "        accepted = list(self.accepted)\n",
    "        shadow = ['Max_Shadow','Median_Shadow','Min_Shadow','Mean_Shadow']\n",
    "\n",
    "        tentative_map = self.create_list(tentative, maps[0])\n",
    "        rejected_map  = self.create_list(rejected, maps[1])\n",
    "        accepted_map  = self.create_list(accepted, maps[2])\n",
    "        shadow_map = self.create_list(shadow, maps[3])\n",
    "\n",
    "        values = tentative_map + rejected_map + accepted_map + shadow_map\n",
    "        keys = tentative + rejected + accepted + shadow\n",
    "\n",
    "        return self.to_dictionary(keys, values)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def to_dictionary(list_one, list_two):\n",
    "        return dict(zip(list_one, list_two))\n",
    "\n",
    "\n",
    "\n",
    "def load_data(data_type='classification'):\n",
    "\n",
    "    \"\"\"\n",
    "    Load Example datasets for the user to try out the package\n",
    "    \"\"\"\n",
    "\n",
    "    data_type = data_type.lower()\n",
    "\n",
    "    if data_type == 'classification':\n",
    "        cancer = load_breast_cancer()\n",
    "        X = pd.DataFrame(np.c_[cancer['data'], cancer['target']], columns = np.append(cancer['feature_names'], ['target']))\n",
    "        y = X.pop('target')\n",
    "\n",
    "    elif data_type == 'regression':\n",
    "        boston = load_boston()\n",
    "        X = pd.DataFrame(np.c_[boston['data'], boston['target']], columns = np.append(boston['feature_names'], ['target']))\n",
    "        y = X.pop('target')\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"No data_type was specified, use either 'classification' or 'regression'\")\n",
    "\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = load_data(data_type='regression')\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db41d1fbcec94168976a20bce57fd1ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-14e2a116809a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m Feature_Selector.fit(X=X, y=y, n_trials=100, sample=False,\n\u001b[0;32m     10\u001b[0m                      \u001b[0mtrain_or_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m                      verbose=True)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-7751e1c7ff7e>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, n_trials, random_state, sample, train_or_test, normalize, verbose, stratify)\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCheck_if_chose_train_or_test_and_train_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_feature_import\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mShadow_feature_import\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_importance_history\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m                 \u001b[0mhits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_hits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-7751e1c7ff7e>\u001b[0m in \u001b[0;36mfeature_importance\u001b[1;34m(self, normalize)\u001b[0m\n\u001b[0;32m    606\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimportance_measure\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'shap'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m             \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-7751e1c7ff7e>\u001b[0m in \u001b[0;36mexplain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    699\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m         \u001b[0mexplainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTreeExplainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_perturbation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"tree_path_dependent\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PythonCode\\MetEireann\\Met_Eireann\\lib\\site-packages\\shap\\explainers\\_tree.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model, data, model_output, feature_perturbation, feature_names, **deprecated_options)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_perturbation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_perturbation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpected_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTreeEnsemble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_missing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[1;31m#self.model_output = self.model.model_output # this allows the TreeEnsemble to translate model outputs types by how it loads the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PythonCode\\MetEireann\\Met_Eireann\\lib\\site-packages\\shap\\explainers\\_tree.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model, data, data_missing, model_output)\u001b[0m\n\u001b[0;32m    841\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moriginal_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_booster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xgboost\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 843\u001b[1;33m             \u001b[0mxgb_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXGBTreeModelLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moriginal_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    844\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrees\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_trees\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_missing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    845\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_offset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PythonCode\\MetEireann\\Met_Eireann\\lib\\site-packages\\shap\\explainers\\_tree.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, xgb_model)\u001b[0m\n\u001b[0;32m   1485\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode_cright\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1486\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode_sindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'I'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1487\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'f'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1489\u001b[0m             \u001b[1;31m# load the stat nodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PythonCode\\MetEireann\\Met_Eireann\\lib\\site-packages\\shap\\explainers\\_tree.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1534\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1535\u001b[1;33m         \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalcsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1536\u001b[0m         \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1537\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = XGBRegressor()\n",
    "\n",
    "# no model selected default is Random Forest, if classification is True it is a Classification problem\n",
    "Feature_Selector = BorutaShap(model=model,\n",
    "                              importance_measure='shap',\n",
    "                              classification=False)\n",
    "\n",
    "\n",
    "Feature_Selector.fit(X=X, y=y, n_trials=100, sample=False,\n",
    "                     train_or_test = 'test', normalize=True,\n",
    "                     verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Feature importance is not defined for Booster type None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-f9dc4bf4ced8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\PythonCode\\MetEireann\\Met_Eireann\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfeature_importances_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    738\u001b[0m             raise AttributeError(\n\u001b[0;32m    739\u001b[0m                 \u001b[1;34m'Feature importance is not defined for Booster type {}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 740\u001b[1;33m                 .format(self.booster))\n\u001b[0m\u001b[0;32m    741\u001b[0m         \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_booster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimportance_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimportance_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Feature importance is not defined for Booster type None"
     ]
    }
   ],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-2020_001-365_HL_TSA_SEN2L_TCG_STM_B0003_BHT\n",
      "2020-2020_001-365_HL_TSA_SEN2L_BNR_STM_B0002_CLS\n",
      "2020-2020_001-365_HL_TSA_SEN2L_NDB_STM_B0011_CLS\n",
      "2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0002_CLS\n",
      "2020-2020_001-365_HL_TSA_SEN2L_RE1_STM_B0001_CLS\n",
      "2020-2020_001-365_HL_TSA_SEN2L_TCB_STM_B0012_CLS\n",
      "2020-2020_001-365_HL_TSA_SEN2L_NDB_STM_B0007_DIL\n",
      "2020-2020_001-365_HL_TSA_SEN2L_NDW_STM_B0001_DIL\n",
      "2020-2020_001-365_HL_TSA_SEN2L_NDW_STM_B0004_DIL\n",
      "2020-2020_001-365_HL_TSA_SEN2L_TCB_STM_B0009_DIL\n",
      "2020-2020_001-365_HL_TSA_SEN2L_BNR_STM_B0004_ERO\n",
      "2020-2020_001-365_HL_TSA_SEN2L_BNR_STM_B0005_ERO\n",
      "2020-2020_001-365_HL_TSA_SEN2L_GRN_STM_B0004_ERO\n",
      "2020-2020_001-365_HL_TSA_SEN2L_NDV_STM_B0007_ERO\n",
      "2020-2020_001-365_HL_TSA_SEN2L_NDW_STM_B0013_ERO\n",
      "2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0001_ERO\n",
      "2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0002_ERO\n",
      "2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0003_ERO\n",
      "2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0004_ERO\n",
      "2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0007_ERO\n",
      "2020-2020_001-365_HL_TSA_SEN2L_RE1_STM_B0011_ERO\n",
      "2020-2020_001-365_HL_TSA_SEN2L_RE2_STM_B0008_ERO\n",
      "2020-2020_001-365_HL_TSA_SEN2L_SW1_STM_B0002_ERO\n",
      "2020-2020_001-365_HL_TSA_SEN2L_SW1_STM_B0003_ERO\n",
      "2020-2020_001-365_HL_TSA_SEN2L_SW1_STM_B0004_ERO\n",
      "2020-2020_001-365_HL_TSA_SEN2L_SW1_STM_B0007_ERO\n",
      "2020-2020_001-365_HL_TSA_SEN2L_SW2_STM_B0002_ERO\n",
      "2020-2020_001-365_HL_TSA_SEN2L_TCB_STM_B0004_ERO\n",
      "2020-2020_001-365_HL_TSA_SEN2L_TCB_STM_B0013_ERO\n",
      "2020-2020_001-365_HL_TSA_SEN2L_TCG_STM_B0006_ERO\n",
      "2020-2020_001-365_HL_TSA_SEN2L_TCW_STM_B0012_ERO\n",
      "2020-2020_001-365_HL_TSA_SEN2L_GRN_STM_B0004_GRD\n",
      "2020-2020_001-365_HL_TSA_SEN2L_NDV_STM_B0007_GRD\n",
      "2020-2020_001-365_HL_TSA_SEN2L_RE2_STM_B0011_GRD\n",
      "2020-2020_001-365_HL_TSA_SEN2L_TCW_STM_B0007_GRD\n",
      "2020-2020_001-365_HL_TSA_SEN2L_BNR_STM_B0004_OPN\n",
      "2020-2020_001-365_HL_TSA_SEN2L_NDW_STM_B0013_OPN\n",
      "2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0012_OPN\n",
      "2020-2020_001-365_HL_TSA_SEN2L_RE1_STM_B0011_OPN\n",
      "2020-2020_001-365_HL_TSA_SEN2L_SW1_STM_B0012_OPN\n",
      "2020-2020_001-365_HL_TSA_SEN2L_TCG_STM_B0003_OPN\n",
      "2020-2020_001-365_HL_TSA_SEN2L_TCG_STM_B0008_OPN\n",
      "2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0001_CLS\n",
      "2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0007_CLS\n",
      "2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0011_CLS\n",
      "2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0012_CLS\n",
      "2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0001_CLS\n",
      "2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0006_CLS\n",
      "2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0011_CLS\n",
      "2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0001_DIL\n",
      "2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0002_DIL\n",
      "2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0003_DIL\n",
      "2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0007_DIL\n",
      "2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0012_DIL\n",
      "2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0001_DIL\n",
      "2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0003_DIL\n",
      "2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0005_DIL\n",
      "2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0007_DIL\n",
      "2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0012_DIL\n",
      "2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0003_ERO\n",
      "2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0009_ERO\n",
      "2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0001_GRD\n",
      "2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0005_GRD\n",
      "2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0012_GRD\n",
      "2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0002_GRD\n",
      "2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0003_GRD\n",
      "2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0004_GRD\n",
      "2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0006_GRD\n",
      "2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0007_GRD\n",
      "2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0008_GRD\n",
      "2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0001_OPN\n",
      "2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0005_OPN\n",
      "2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0007_OPN\n",
      "2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0011_OPN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[-9188, -7659,   782, ...,  -272,  1627,  2015],\n",
       "        [-9188, -5349,   855, ...,  -190,  1570,  1930],\n",
       "        [-9188, -5349,   959, ...,  -171,  1541,  1876],\n",
       "        ...,\n",
       "        [-9553, -7041,  1186, ...,  -135,  -350,  -294],\n",
       "        [-9553, -7041,  1186, ...,  -135,  -350,  -294],\n",
       "        [-9553, -7041,  1111, ...,   -33,  -354,  -294]],\n",
       "\n",
       "       [[-9671, -7659,   782, ...,  -254,  1570,  1930],\n",
       "        [-9357, -6336,   959, ...,  -190,  1541,  1876],\n",
       "        [-9357, -5349,  1033, ...,  -171,  1361,  1772],\n",
       "        ...,\n",
       "        [-9553, -7041,  1186, ...,  -135,  -351,  -294],\n",
       "        [-9553, -7041,  1186, ...,  -135,  -354,  -294],\n",
       "        [-9553, -7041,  1111, ...,   -33,  -363,  -294]],\n",
       "\n",
       "       [[-9999, -7659,   959, ...,  -254,  1541,  1876],\n",
       "        [-9999, -6336,  1346, ...,  -190,  1361,  1772],\n",
       "        [-9999, -5349,  1431, ...,  -190,  1241,  1724],\n",
       "        ...,\n",
       "        [-9553, -7041,  1186, ...,  -132,  -358,  -294],\n",
       "        [-9553, -7041,  1234, ...,  -132,  -363,  -294],\n",
       "        [-9553, -7041,  1111, ...,  -135,  -361,  -294]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-9999, -9999, -9999, ...,   351,  1031,  1658],\n",
       "        [-9999, -9999, -9999, ...,   351,  1031,  1658],\n",
       "        [-9999, -9999, -9999, ...,   351,   961,  1658],\n",
       "        ...,\n",
       "        [-9999, -9999, -9999, ...,  -269,   239,   296],\n",
       "        [-9999, -9999, -9999, ...,  -172,   233,   296],\n",
       "        [-9999, -9999, -9999, ...,   -82,   204,   262]],\n",
       "\n",
       "       [[-9999, -9999, -9999, ...,   351,  1031,  1665],\n",
       "        [-9999, -9999, -9999, ...,   351,  1031,  1658],\n",
       "        [-9999, -9999, -9999, ...,   351,  1031,  1658],\n",
       "        ...,\n",
       "        [-9999, -9999, -9999, ...,   -88,   269,   389],\n",
       "        [-9999, -9999, -9999, ...,   -36,   259,   387],\n",
       "        [-9999, -9999, -9999, ...,   -20,   239,   324]],\n",
       "\n",
       "       [[-9999, -9999, -9999, ...,   228,  1031,  1665],\n",
       "        [-9999, -9999, -9999, ...,   351,  1031,  1658],\n",
       "        [-9999, -9999, -9999, ...,   351,  1031,  1658],\n",
       "        ...,\n",
       "        [-9999, -9999, -9999, ...,   -20,   269,   415],\n",
       "        [-9999, -9999, -9999, ...,    96,   269,   415],\n",
       "        [-9999, -9999, -9999, ...,   125,   269,   389]]], dtype=int16)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from osgeo import gdal\n",
    "import glob\n",
    "\n",
    "filepath_2 = 'C:/Users/egnke/PythonCode/MetEireann/Sentienl-2-Data/Processed_Data/morphology/X0002_Y0002/'\n",
    "filepath_1 = 'C:/Users/egnke/PythonCode/MetEireann/Sentinel-1-Data/Sentinel-1/Texture/Asc/X0002_Y0003/'\n",
    "\n",
    "file_ = 'TEXTURE_HL_TXT_BHT.tif'\n",
    "\n",
    "\n",
    "def extract_morphology_operater(filepath):\n",
    "    '''\n",
    "    exctracts the morphology operator from the file name\n",
    "    'C:/home/ubuntu/TEXTURE_HL_TXT_BHT.tif' -> _BHT\n",
    "    need to remove this when comapring features to band description\n",
    "    '''\n",
    "    file_name = os.path.basename(filepath)\n",
    "    clean_name = file_name.replace('.tif','')\n",
    "    \n",
    "    return '_'+clean_name.split('_')[-1]\n",
    "\n",
    "\n",
    "def stack_images_into_volume(list_of_img_rasters):\n",
    "    '''\n",
    "    stacks numpy arrays together along z-axis to create\n",
    "    multichannel image.\n",
    "    '''\n",
    "    return np.dstack(list_of_img_rasters)\n",
    "\n",
    "\n",
    "def extract_bands(directory, img_bands, important_features):\n",
    "    '''\n",
    "    given a list of band names extract the relevant tif files from the directory \n",
    "    and stack them into a mutli channel image.\n",
    "    '''\n",
    "    \n",
    "\n",
    "    for tif in glob.glob(directory + \"/*.tif\"):\n",
    "        \n",
    "        filepath = tif.replace('\\\\','/')\n",
    "        morph = extract_morphology_operater(filepath)\n",
    "    \n",
    "        gdal_file = gdal.Open(filepath)\n",
    "                \n",
    "        for band in range(gdal_file.RasterCount):\n",
    "            band += 1\n",
    "            gdal_band = gdal_file.GetRasterBand(band)\n",
    "            name = gdal_band.GetDescription() + morph\n",
    "\n",
    "            if any(feat in name for feat in important_features):\n",
    "                raster_band = gdal_band.ReadAsArray()\n",
    "                img_bands.append(raster_band)\n",
    "                \n",
    "    return img_bands\n",
    "\n",
    "\n",
    "def extract_bands_and_merge(sentinel_1_asc_dir, sentinel_2_dir, important_features):\n",
    "    \n",
    "    img_band_sent_2 = []\n",
    "    img_band_sent_1 = []\n",
    "    \n",
    "    img_band_sent_2 = extract_bands(sentinel_2_dir, \n",
    "                                    img_band_sent_2, \n",
    "                                    important_features)\n",
    "    \n",
    "    img_band_sent_1 = extract_bands(sentinel_1_asc_dir, \n",
    "                                    img_band_sent_1,\n",
    "                                    important_features)\n",
    "    \n",
    "    \n",
    "    img = img_band_sent_1 + img_band_sent_2\n",
    "    \n",
    "    return stack_images_into_volume(img)\n",
    "\n",
    "\n",
    "extract_bands_and_merge(filepath_1, filepath_2, important_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = ['2020-2020_001-365_HL_TSA_SEN2L_NDV_STM_B0007_GRD', '2020-2020_001-365_HL_TSA_SEN2L_NDV_STM_B0007_ERO', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0011_OPN', '2020-2020_001-365_HL_TSA_SEN2L_BNR_STM_B0002_CLS', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0003_DIL', '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0003_GRD', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0011_CLS', '2020-2020_001-365_HL_TSA_SEN2L_TCG_STM_B0008_OPN', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_BNR_STM_B0004_OPN', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0001_DIL', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0012_DIL', '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0006_GRD', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_SW1_STM_B0002_ERO', '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0007_DIL', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0002_GRD', '2020-2020_001-365_HL_TSA_SEN2L_NDW_STM_B0001_DIL', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_BNR_STM_B0005_ERO', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0005_GRD', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_GRN_STM_B0004_ERO', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0002_DIL', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0012_DIL', '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0001_DIL', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_SW1_STM_B0012_OPN', '2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0003_ERO', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0003_ERO', '2020-2020_001-365_HL_TSA_SEN2L_TCW_STM_B0012_ERO', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0005_DIL', '2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0007_ERO', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_BNR_STM_B0004_ERO', '2020-2020_001-365_HL_TSA_SEN2L_TCG_STM_B0006_ERO', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0005_OPN', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0007_DIL', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_NDB_STM_B0007_DIL', '2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0004_ERO', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_NDW_STM_B0013_ERO', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0001_CLS', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_GRN_STM_B0004_GRD', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0001_OPN', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_NDW_STM_B0004_DIL', '2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0012_OPN', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_SW2_STM_B0002_ERO', '2020-2020_001-365_HL_TSA_SEN2L_TCB_STM_B0009_DIL', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0002_ERO', '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0006_CLS', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_NDW_STM_B0013_OPN', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0003_DIL', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_TCW_STM_B0007_GRD', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0011_CLS', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0012_CLS', '2020-2020_001-365_HL_TSA_SEN2L_RE2_STM_B0011_GRD', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0004_GRD', '2020-2020_001-365_HL_TSA_SEN2L_TCB_STM_B0012_CLS', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0008_GRD', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0012_GRD', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_RE1_STM_B0001_CLS', '2020-2020_001-365_HL_TSA_SEN2L_TCG_STM_B0003_OPN', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_TCG_STM_B0003_BHT', '2020-2020_001-365_HL_TSA_SEN2L_SW1_STM_B0003_ERO', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0002_CLS', '2020-2020_001-365_HL_TSA_SEN2L_NDB_STM_B0011_CLS', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0009_ERO', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0007_CLS', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_SW1_STM_B0007_ERO', '2020-2020_001-365_HL_TSA_SEN2L_TCB_STM_B0013_ERO', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_SW1_STM_B0004_ERO', '2020-2020_001-365_HL_TSA_SEN2L_RE1_STM_B0011_ERO', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0001_GRD', '2020-2020_001-365_HL_TSA_SEN2L_RE1_STM_B0011_OPN', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0001_CLS', '2020-2020_001-365_HL_TSA_SEN2L_TCB_STM_B0004_ERO', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0001_ERO', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0007_OPN', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0007_GRD', '2020-2020_001-365_HL_TSA_SEN2L_RE2_STM_B0008_ERO']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(important_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32+42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "met_eireann",
   "language": "python",
   "name": "met_eireann"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
