{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bded360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nimages on the borders are clogging up machine\\npadding isn't working as the self array samples is coming from unpadded \\nso all the coordinates are off.\\n\\n1. pad the height (y) data aswell so the coordinates are the same\\n\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import keras\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "'''\n",
    "\n",
    "images on the borders are clogging up machine\n",
    "padding isn't working as the self array samples is coming from unpadded \n",
    "so all the coordinates are off.\n",
    "\n",
    "1. pad the height (y) data aswell so the coordinates are the same\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c61b049",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "\n",
    "    \n",
    "    def __init__(self, batch_size=32, random_state=42, train_test='train', offset=5,\n",
    "                 height_dir='C:/Users/egnke/PythonCode/MetEireann/Dublin_Height_Data/tiled/',\n",
    "                 sentinel_2_dir='C:/Users/egnke/PythonCode/MetEireann/Sentienl-2-Data/Processed_Data/interpolation/',\n",
    "                 sentinel_1_dir='C:/Users/egnke/PythonCode/MetEireann/Sentinel-1-Data/Sentinel-1/Interpolation/Asc/'):\n",
    "        \n",
    "        \n",
    "        self.offset = offset\n",
    "        self.batch_size = batch_size\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        if train_test == 'train':\n",
    "            \n",
    "            self.tiles  = ['X0002_Y0002','X0002_Y0003','X0003_Y0002']\n",
    "        else:\n",
    "            \n",
    "            self.tiles   = ['X0003_Y0003']\n",
    "        \n",
    "        self.bands = ['2020-2020_001-365_HL_TSA_SEN2L_NDV_STM.tif','2020-2020_001-365_HL_TSA_VVVHP_BVH_STM.tif',\n",
    "                      '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM.tif','2020-2020_001-365_HL_TSA_SEN2L_BNR_STM.tif',\n",
    "                      '2020-2020_001-365_HL_TSA_SEN2L_SW1_STM.tif','2020-2020_001-365_HL_TSA_SEN2L_NDW_STM.tif',\n",
    "                      '2020-2020_001-365_HL_TSA_SEN2L_TCG_STM.tif']\n",
    "        \n",
    "        self.height_dir = height_dir\n",
    "        self.sentinel_2_dir = sentinel_2_dir\n",
    "        self.sentinel_1_dir = sentinel_1_dir\n",
    "        \n",
    "        \n",
    "        self.scene_df = self.create_sampling_array()\n",
    "        print('finshed creating scene df....')\n",
    "        self.volumes = self.create_img_volumes(self.tiles)\n",
    "        print('finshed creating image volumes....')\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    \n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        '''Updates indexes after each epoch'''\n",
    "        self.scene_df = self.scene_df.sample(frac = 1)\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        '''Denotes the number of batches per epoch'''\n",
    "        return int(np.floor(len(self.scene_df) /(self.batch_size*10)))\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        '''Generate one batch of data'''\n",
    "        # Generate indexes of the batch\n",
    "        sample_ = self.scene_df.sample(n=self.batch_size)\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(sample_)\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    \n",
    "    def __data_generation(self, sample_df):\n",
    "        \n",
    "        X = np.empty((self.batch_size, *(11,11), 7))\n",
    "        y = np.empty((self.batch_size, 1))\n",
    "        \n",
    "        \n",
    "        X_temp, y_temp = zip(*sample_df.apply(self.extract_x_y_from_volumes, volume_dict=self.volumes, offset=2, axis=1))\n",
    "        \n",
    "\n",
    "        X = np.stack(X_temp, axis=0)\n",
    "        y = np.stack(y_temp, axis=0)\n",
    "        \n",
    "        X = X.astype(np.float32)\n",
    "        y = y.astype(np.float32)\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    \n",
    "         \n",
    "    def create_sampling_array(self):\n",
    "        \n",
    "        dfs = []\n",
    "        for tile in self.tiles:\n",
    "            \n",
    "            height = gdal.Open(self.height_dir+tile+'/IE001L1_Dublin_UA2012_DHM_v010.tif')\n",
    "            height_data = height.GetRasterBand(1).ReadAsArray()\n",
    " \n",
    "            height_data = np.pad(height_data, pad_width=((6, 6), (6, 6)), \n",
    "                                 mode='constant',\n",
    "                                 constant_values=-9999)\n",
    "            \n",
    "            \n",
    "            height_df = self.convert_2d_array_to_dataframe(height_data, 'heights')\n",
    "            height_df = height_df[height_df['heights'] > 0]\n",
    "            height_df['tile'] = tile\n",
    "            \n",
    "            dfs.append(height_df)\n",
    "            \n",
    "        return pd.concat(dfs)\n",
    "        \n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def convert_2d_array_to_dataframe(raster, colname):\n",
    "    \n",
    "        '''\n",
    "        converts a raster to dataframe where each pixel value is a row and the x y coordinates are there\n",
    "\n",
    "        np.array([[1,2,3],[4,5,6],[7,8,9]]) ->  y,x,value\n",
    "                                                0,0,1\n",
    "                                                0,1,2\n",
    "                                                0,2,3\n",
    "                                                1,0,4\n",
    "                                                1,1,5\n",
    "                                                1,2,6\n",
    "                                                2,0,7\n",
    "                                                2,1,8\n",
    "                                                2,2,9\n",
    "        '''\n",
    "\n",
    "        return (pd.DataFrame(raster)\n",
    "                 .stack()\n",
    "                 .rename_axis(['y', 'x'])\n",
    "                 .reset_index(name=colname))\n",
    "    \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def stack_images_into_volume(list_of_img_rasters):\n",
    "        '''\n",
    "        stacks numpy arrays together along z-axis to create\n",
    "        multichannel image.\n",
    "        '''\n",
    "        return np.dstack(list_of_img_rasters)\n",
    "\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def zero_pad_img(img, before_after_axis=((6, 6), (6, 6), (0, 0))):\n",
    "        '''\n",
    "        zero pad numpy array default will pad 250x250x3 -> 256x256x3\n",
    "        each tuple in tuple refers to an axis ((3,3),(3,3),(0,0))\n",
    "        so if we wanted to convert a 250x250x250x3 -> 256x256x256x3\n",
    "        we would write ((3,3),(3,3),(3,3),(0,0))\n",
    "        '''\n",
    "        \n",
    "        padded_img = np.pad(img, before_after_axis, \n",
    "                            mode='constant', constant_values=-9999)\n",
    "        return padded_img\n",
    "    \n",
    "    \n",
    "    \n",
    "    def create_img_volume(self, tile):\n",
    "        \n",
    "        band_list = []\n",
    "        \n",
    "        for band in self.bands:\n",
    "            \n",
    "            # check for sentinel 1 band\n",
    "            if 'VVVHP_' in band:\n",
    "                source = gdal.Open(self.sentinel_1_dir + tile + '/' + band)\n",
    "                band_data = source.GetRasterBand(7).ReadAsArray()\n",
    "                band_list.append(band_data)\n",
    "                \n",
    "            else:\n",
    "                source = gdal.Open(self.sentinel_2_dir + tile + '/' + band)\n",
    "                band_data = source.GetRasterBand(7).ReadAsArray()\n",
    "                band_list.append(band_data)\n",
    "                \n",
    "        return self.stack_images_into_volume(band_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def extract_x_y_from_volumes(self, row, volume_dict, offset):\n",
    "        img_volume = volume_dict[row['tile']]\n",
    "        x = row['y']\n",
    "        y = row['x']\n",
    "        #x = row['x']\n",
    "        #y = row['y']\n",
    "        return img_volume[x-self.offset:x+self.offset+1, y-self.offset:y+self.offset+1], row['heights']\n",
    "    \n",
    "    \n",
    "    \n",
    "    def create_img_volumes(self, tiles):\n",
    "        \n",
    "        volumes = {}\n",
    "        for tile in tiles:\n",
    "            volume = self.create_img_volume(tile)\n",
    "            volume = self.zero_pad_img(volume)\n",
    "            volumes[tile] = volume\n",
    "            \n",
    "        return volumes\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10d643f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 9, 9, 6)           384       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 4, 4, 6)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 4, 4, 6)           24        \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 4, 4, 6)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 96)                384       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 889\n",
      "Trainable params: 685\n",
      "Non-trainable params: 204\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    model.add(keras.layers.Conv2D(filters=6, kernel_size=(3, 3), activation='linear', input_shape=(11, 11, 7)))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    \n",
    "\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add(keras.layers.Dense(units=1, activation = 'linear'))\n",
    "    \n",
    "              \n",
    "    return model\n",
    "    \n",
    "    \n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "\n",
    "Model = create_model()\n",
    "Model.compile(optimizer='adam',  run_eagerly=True, loss=tf.keras.losses.MeanAbsoluteError(), metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d77414e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finshed creating scene df....\n",
      "finshed creating image volumes....\n",
      "finshed creating scene df....\n",
      "finshed creating image volumes....\n"
     ]
    }
   ],
   "source": [
    "train_generator = DataGenerator(train_test='train')\n",
    "test_generator  = DataGenerator(train_test='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18bb927a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2669/2669 [==============================] - 150s 56ms/step - loss: 3.2414 - root_mean_squared_error: 4.8256 - val_loss: 1.5346 - val_root_mean_squared_error: 2.4698\n",
      "Epoch 2/100\n",
      "2669/2669 [==============================] - 136s 51ms/step - loss: 2.0365 - root_mean_squared_error: 3.4954 - val_loss: 1.5459 - val_root_mean_squared_error: 2.7449\n",
      "Epoch 3/100\n",
      "2669/2669 [==============================] - 116s 43ms/step - loss: 1.9730 - root_mean_squared_error: 3.3770 - val_loss: 1.5124 - val_root_mean_squared_error: 2.3491\n",
      "Epoch 4/100\n",
      "2669/2669 [==============================] - 125s 47ms/step - loss: 1.9757 - root_mean_squared_error: 3.3887 - val_loss: 1.5530 - val_root_mean_squared_error: 2.4867\n",
      "Epoch 5/100\n",
      "2669/2669 [==============================] - 118s 44ms/step - loss: 1.9403 - root_mean_squared_error: 3.3285 - val_loss: 1.5048 - val_root_mean_squared_error: 2.4574\n",
      "Epoch 6/100\n",
      "2669/2669 [==============================] - 122s 46ms/step - loss: 1.9233 - root_mean_squared_error: 3.3458 - val_loss: 1.5737 - val_root_mean_squared_error: 2.6412\n",
      "Epoch 7/100\n",
      "2669/2669 [==============================] - 121s 45ms/step - loss: 1.9380 - root_mean_squared_error: 3.3643 - val_loss: 1.5002 - val_root_mean_squared_error: 2.4466\n",
      "Epoch 8/100\n",
      "2669/2669 [==============================] - 124s 46ms/step - loss: 1.9502 - root_mean_squared_error: 3.3424 - val_loss: 1.5065 - val_root_mean_squared_error: 2.3043\n",
      "Epoch 9/100\n",
      "2669/2669 [==============================] - 520s 195ms/step - loss: 1.9249 - root_mean_squared_error: 3.3164 - val_loss: 1.5363 - val_root_mean_squared_error: 2.4054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b52e42aec8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback = keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "Model.fit(\n",
    "            train_generator,\n",
    "            epochs=100,\n",
    "            validation_data=test_generator,\n",
    "            workers=2, use_multiprocessing=False,\n",
    "            callbacks=[callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66e0dfb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_features = ['2020-2020_001-365_HL_TSA_SEN2L_NDV_STM_B0007_GRD', '2020-2020_001-365_HL_TSA_SEN2L_NDV_STM_B0007_ERO', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0011_OPN', '2020-2020_001-365_HL_TSA_SEN2L_BNR_STM_B0002_CLS', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0003_DIL', '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0003_GRD', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0011_CLS', '2020-2020_001-365_HL_TSA_SEN2L_TCG_STM_B0008_OPN', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_BNR_STM_B0004_OPN', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0001_DIL', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0012_DIL', '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0006_GRD', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_SW1_STM_B0002_ERO', '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0007_DIL', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0002_GRD', '2020-2020_001-365_HL_TSA_SEN2L_NDW_STM_B0001_DIL', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_BNR_STM_B0005_ERO', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0005_GRD', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_GRN_STM_B0004_ERO', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0002_DIL', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0012_DIL', '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0001_DIL', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_SW1_STM_B0012_OPN', '2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0003_ERO', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0003_ERO', '2020-2020_001-365_HL_TSA_SEN2L_TCW_STM_B0012_ERO', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0005_DIL', '2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0007_ERO', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_BNR_STM_B0004_ERO', '2020-2020_001-365_HL_TSA_SEN2L_TCG_STM_B0006_ERO', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0005_OPN', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0007_DIL', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_NDB_STM_B0007_DIL', '2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0004_ERO', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_NDW_STM_B0013_ERO', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0001_CLS', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_GRN_STM_B0004_GRD', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0001_OPN', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_NDW_STM_B0004_DIL', '2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0012_OPN', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_SW2_STM_B0002_ERO', '2020-2020_001-365_HL_TSA_SEN2L_TCB_STM_B0009_DIL', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0002_ERO', '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0006_CLS', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_NDW_STM_B0013_OPN', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0003_DIL', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_TCW_STM_B0007_GRD', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0012_CLS', '2020-2020_001-365_HL_TSA_SEN2L_RE2_STM_B0011_GRD', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0004_GRD', '2020-2020_001-365_HL_TSA_SEN2L_TCB_STM_B0012_CLS', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0008_GRD', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0012_GRD', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_RE1_STM_B0001_CLS', '2020-2020_001-365_HL_TSA_SEN2L_TCG_STM_B0003_OPN', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_TCG_STM_B0003_BHT', '2020-2020_001-365_HL_TSA_SEN2L_SW1_STM_B0003_ERO', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0002_CLS', '2020-2020_001-365_HL_TSA_SEN2L_NDB_STM_B0011_CLS', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0009_ERO', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0007_CLS', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_SW1_STM_B0007_ERO', '2020-2020_001-365_HL_TSA_SEN2L_TCB_STM_B0013_ERO', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_SW1_STM_B0004_ERO', '2020-2020_001-365_HL_TSA_SEN2L_RE1_STM_B0011_ERO', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0001_GRD', '2020-2020_001-365_HL_TSA_SEN2L_RE1_STM_B0011_OPN', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0001_CLS', '2020-2020_001-365_HL_TSA_SEN2L_TCB_STM_B0004_ERO', \n",
    "                     '2020-2020_001-365_HL_TSA_SEN2L_NIR_STM_B0001_ERO', '2020-2020_001-365_HL_TSA_VVVHP_BVV_STM_B0007_OPN', \n",
    "                     '2020-2020_001-365_HL_TSA_VVVHP_BVH_STM_B0007_GRD', '2020-2020_001-365_HL_TSA_SEN2L_RE2_STM_B0008_ERO']\n",
    "\n",
    "\n",
    "len(important_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb90fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_points(size, chunk_size, overlap=0):\n",
    "    '''\n",
    "    split an array into equal parts including overlap. used\n",
    "    as the starting points then we add the size to each point to get\n",
    "    the desired result.\n",
    "    \n",
    "    chunk_size: is the size of the stride length to take \n",
    "                i.e chunk_size = 2 [0,1,2,3] -> [0,2]\n",
    "    overlap: is a float 0.5=50% of the overlap to take between chunk_sizes\n",
    "            chunk_size:4, np.arange(10), overlap:0.5 [0, 4, 6] -> [0, 2, 4, 6]\n",
    "    '''\n",
    "    \n",
    "    if(overlap>1 or overlap<0): raise ValueError('The Overlap Parameter must be between 0-1!')\n",
    "    \n",
    "    points = [0]\n",
    "    stride = int(chunk_size * (1-overlap))\n",
    "    counter = 1\n",
    "    while True:\n",
    "        \n",
    "        pt = stride * counter\n",
    "        \n",
    "        if pt + chunk_size >= size:\n",
    "            points.append(size - chunk_size)\n",
    "            break\n",
    "        else:\n",
    "            points.append(pt)\n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "    return points\n",
    "\n",
    "\n",
    "chunk_size = 2\n",
    "x = np.array([0,1,2,3])\n",
    "start_points(len(x), chunk_size, overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8a70f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "met_eireann",
   "language": "python",
   "name": "met_eireann"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
